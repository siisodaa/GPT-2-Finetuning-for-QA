# -*- coding: utf-8 -*-
"""Training Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qtvdOJSaAQZooQzhS6qkOmmCxW58jgyM
"""

import json
import logging
import os
import random
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset, Dataset
import pandas as pd

# Setup logging
logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
                    datefmt='%m/%d/%Y %H:%M:%S',
                    level=logging.INFO)
logger = logging.getLogger(__name__)

# Parameters
output_dir = "./gpt2"
model_name_or_path = "gpt2-medium"
max_seq_length = 512
max_answer_length = 128
num_train_epochs = 3
train_batch_size = 2
learning_rate = 5e-5
warmup_steps = 5000
weight_decay = 0.05
logging_steps = 1000
save_strategy = "no"
gradient_accumulation_steps = 4
train_subset_size = 1000
validation_subset_size = 10
seed = 42

def set_seed(seed):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def preprocess_data(examples):
    context = examples['document']['summary'] if 'document' in examples and 'summary' in examples['document'] else ""
    question = examples['question'] if 'question' in examples else ""
    answers = examples['answers']['text'] if 'answers' in examples and 'text' in examples['answers'] else []

    processed_data = []
    for answer in answers:
        processed_data.append({
            "context": context,
            "question": question,
            "answer": answer
        })
    return processed_data

def tokenize_function(examples, tokenizer, max_seq_length, max_answer_length):
    questions_contexts = ["question: " + str(q) + " context: " + str(c) for q, c in zip(examples['question'], examples['context'])]
    model_inputs = tokenizer(questions_contexts, truncation=True, padding="max_length", max_length=max_seq_length)
    labels = tokenizer(examples['answer'], truncation=True, padding="max_length", max_length=max_answer_length)
    model_inputs['labels'] = labels['input_ids']
    return model_inputs

def main():
    set_seed(seed)

    # Load and parse the NarrativeQA dataset
    logger.info("Loading and parsing NarrativeQA datasets.")
    narrativeqa = load_dataset("narrativeqa")

    # Preprocess the data
    logger.info("Preprocessing data.")
    train_data = narrativeqa['train'].select(range(train_subset_size))
    validation_data = narrativeqa['validation'].select(range(validation_subset_size))

    train_data = train_data.map(preprocess_data, batched=True, remove_columns=train_data.column_names)
    validation_data = validation_data.map(preprocess_data, batched=True, remove_columns=validation_data.column_names)

    train_df = pd.DataFrame(train_data)
    validation_df = pd.DataFrame(validation_data)

    train_dataset = Dataset.from_pandas(train_df)
    validation_dataset = Dataset.from_pandas(validation_df)

    # Tokenizer initialization
    tokenizer = GPT2Tokenizer.from_pretrained(model_name_or_path)

    # Add padding token
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    # Tokenize the data
    logger.info("Tokenizing data.")
    tokenized_train_dataset = train_dataset.map(lambda x: tokenize_function(x, tokenizer, max_seq_length, max_answer_length), batched=True)
    tokenized_validation_dataset = validation_dataset.map(lambda x: tokenize_function(x, tokenizer, max_seq_length, max_answer_length), batched=True)

    # Load model
    model = GPT2LMHeadModel.from_pretrained(model_name_or_path)
    model.resize_token_embeddings(len(tokenizer))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=train_batch_size,
        warmup_steps=warmup_steps,
        weight_decay=weight_decay,
        learning_rate=learning_rate,
        logging_dir=os.path.join(output_dir, 'logs'),
        logging_steps=logging_steps,
        save_strategy=save_strategy,
        gradient_accumulation_steps=gradient_accumulation_steps,
        fp16=torch.cuda.is_available(),
        report_to="none"
    )

    # Data collator
    data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

    # Initialize Trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_train_dataset
    )

    # Custom training loop to evaluate the model after each epoch
    for epoch in range(num_train_epochs):
        logger.info(f"Starting epoch {epoch + 1}/{num_train_epochs}")
        trainer.train()

        # Evaluation on validation dataset
        logger.info("Evaluating on validation dataset.")
        model.eval()
        for example in validation_data:
            input_text = f"Context: {example['context']} Question: {example['question']}"
            inputs = tokenizer(input_text, return_tensors="pt", max_length=max_seq_length, truncation=True, padding="max_length").to(device)
            output = model.generate(**inputs, max_length=max_answer_length, num_return_sequences=1)
            answer = tokenizer.decode(output[0], skip_special_tokens=True)
            logger.info(f"Context: {example['context']}")
            logger.info(f"Question: {example['question']}")
            logger.info(f"Generated Answer: {answer}")
            logger.info(f"Actual Answer: {example['answer']}")

    # Save the trained model and tokenizer
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)

    logger.info("Model and tokenizer saved successfully.")

if __name__ == "__main__":
    main()